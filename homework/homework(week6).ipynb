{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from hashlib import md5\n",
    "import time\n",
    "\n",
    "def getUrl(url):\n",
    "    response = requests.get(url)\n",
    "    pdfUrl = []\n",
    "    count = 0\n",
    "    bs = BeautifulSoup(response.text,\"html.parser\")\n",
    "    pdfs = bs.find_all(href=re.compile('/content/.*html'))\n",
    "    names = []\n",
    "    for tag in pdfs:\n",
    "        if count < 20: \n",
    "            pdfUrl.append(\"https://openaccess.thecvf.com\"+tag.get(\"href\"))\n",
    "            count = count+1\n",
    "            names.append(re.split('[./]', tag.get(\"href\"))[-2][:-16].replace(\"_\",\" \"))\n",
    "        else: break\n",
    "    return names,pdfUrl\n",
    "\n",
    "def getAbstract(paperUrls):\n",
    "    abstract = []\n",
    "    for url in paperUrls:\n",
    "        response = requests.get(url)\n",
    "        bs = BeautifulSoup(response.text,\"html.parser\")\n",
    "        abstract.append(bs.find(id=\"abstract\").text.strip())\n",
    "    return abstract\n",
    "\n",
    "def translate_api(input_text):\n",
    "\n",
    "    appid = '20231227001922823'\n",
    "    appkey = 'SaOY2MMYDWqj6TgCD79O'\n",
    "    def make_md5(s, encoding='utf-8'):\n",
    "        return md5(s.encode(encoding)).hexdigest()\n",
    "\n",
    "    from_lang = 'en'\n",
    "    to_lang = 'zh'\n",
    "    endpoint = 'http://api.fanyi.baidu.com'\n",
    "    path = '/api/trans/vip/translate'\n",
    "    url = endpoint + path\n",
    "    \n",
    "    query = input_text\n",
    "\n",
    "    salt = random.randint(32768, 65536)\n",
    "    sign = make_md5(appid + query + str(salt) + appkey)\n",
    "\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    payload = {'appid': appid, 'q': query, 'from': from_lang, 'to': to_lang, 'salt': salt, 'sign': sign}\n",
    "\n",
    "    r = requests.post(url, params=payload, headers=headers)\n",
    "    result = r.json()\n",
    "    return result\n",
    "\n",
    "\n",
    "def write2txt(path,title,contents):\n",
    "    with open(path,\"w\") as f:\n",
    "        for ti,con in zip(title,contents):\n",
    "            f.write(ti+\"\\n\")\n",
    "            print(con)\n",
    "            f.write(con+\"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "web = \"https://openaccess.thecvf.com/ICCV2021?day=2021-10-12\"\n",
    "name,pdfurls = getUrl(web)\n",
    "abstracts = getAbstract(pdfurls)\n",
    "ch_abstracts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于学习的图像去噪方法已经局限于这样的情况，即给出了对齐良好的有噪声和干净的图像，或者从预定的噪声模型（例如，高斯）合成样本。尽管最近的生成噪声建模方法旨在模拟真实世界噪声的未知分布，但仍存在一些局限性。在实际场景中，噪声生成器应该学会在不使用成对的噪声和干净图像的情况下模拟一般和复杂的噪声分布。然而，由于现有的方法是基于对真实世界噪声的不切实际的假设构建的，它们往往会生成令人难以置信的模式，并且无法表达复杂的噪声图。因此，我们引入了一种干净到噪声的图像生成框架，即C2N，以在不使用任何配对示例的情况下模拟复杂的真实世界噪声。我们在C2N中相应地构建了具有真实世界噪声特性的每个分量的噪声生成器，以准确地表达宽范围的噪声。综合智慧\n",
      "在现实世界中不断学习必须克服许多挑战，其中嘈杂的标签是一个常见且不可避免的问题。在这项工作中，我们首次提出了一个基于重放的连续学习框架，该框架同时解决了灾难性遗忘和噪声标签。我们的解决方案基于两个观察结果；（i） 即使有噪声标签，也可以通过自监督学习来减轻遗忘，并且（ii）重放缓冲区的纯度至关重要。基于这一点，我们提出了我们方法的两个关键组成部分：（i）一种名为self replay的自监督重放技术，它可以绕过由噪声标记数据产生的错误训练信号，以及（ii）自中心滤波器，它通过基于中心性的随机图集合来维护纯化的重放缓冲区。MNIST、CIFAR-10、CIFAR-100和WebVision在真实世界噪声下的实验结果表明，我们的框架可以在噪声流数据中保持高度纯净的重放缓冲区\n",
      "基于学习的图像去噪方法已经局限于这样的情况，即给出了对齐良好的有噪声和干净的图像，或者从预定的噪声模型（例如，高斯）合成样本。尽管最近的生成噪声建模方法旨在模拟真实世界噪声的未知分布，但仍存在一些局限性。在实际场景中，噪声生成器应该学会在不使用成对的噪声和干净图像的情况下模拟一般和复杂的噪声分布。然而，由于现有的方法是基于对真实世界噪声的不切实际的假设构建的，它们往往会生成令人难以置信的模式，并且无法表达复杂的噪声图。因此，我们引入了一种干净到噪声的图像生成框架，即C2N，以在不使用任何配对示例的情况下模拟复杂的真实世界噪声。我们在C2N中相应地构建了具有真实世界噪声特性的每个分量的噪声生成器，以准确地表达宽范围的噪声。综合智慧\n",
      "在现实世界中不断学习必须克服许多挑战，其中嘈杂的标签是一个常见且不可避免的问题。在这项工作中，我们首次提出了一个基于重放的连续学习框架，该框架同时解决了灾难性遗忘和噪声标签。我们的解决方案基于两个观察结果；（i） 即使有噪声标签，也可以通过自监督学习来减轻遗忘，并且（ii）重放缓冲区的纯度至关重要。基于这一点，我们提出了我们方法的两个关键组成部分：（i）一种名为self replay的自监督重放技术，它可以绕过由噪声标记数据产生的错误训练信号，以及（ii）自中心滤波器，它通过基于中心性的随机图集合来维护纯化的重放缓冲区。MNIST、CIFAR-10、CIFAR-100和WebVision在真实世界噪声下的实验结果表明，我们的框架可以在噪声流数据中保持高度纯净的重放缓冲区\n",
      "我们介绍了一种使用PlenOctrees实时渲染神经辐射场（NeRF）的方法，PlenOctres是一种基于八叉树的3D表示，支持视图相关效果。我们的方法可以以超过150 FPS的速度渲染800x800张图像，这比传统的NeRF快3000多倍。我们在不牺牲质量的情况下这样做，同时保留了NeRF对具有任意几何体和视图相关效果的场景执行自由视点渲染的能力。实时性能是通过将NeRF预先制表为PlenOctree来实现的。为了保持与视图相关的效果，如镜面反射，我们通过闭合形式的球面基函数来分解外观。具体来说，我们证明了训练NeRF来预测辐射的球面谐波表示是可能的，去除了作为神经网络输入的观看方向。此外，我们还证明了PlenOctrees可以直接优化，以进一步最小化重建损失，从而获得相等或更好的qu\n",
      "用于图像语义分割的深度神经网络（DNN）通常被训练为对预定义的对象类的闭合集进行操作。这与DNN被设想部署到的“开放世界”设置形成了鲜明对比。从功能安全的角度来看，检测所谓的“分布外”（OoD）样本（即DNN语义空间之外的对象）的能力对自动驾驶等许多应用至关重要。OoD检测的一种自然基线方法是对像素方向的softmax熵进行阈值设置。我们提出了一个两步程序，大大改进了这种方法。首先，我们利用来自COCO数据集的样本作为OoD代理，并引入第二个训练目标来最大化这些样本的softmax熵。从预先训练的语义分割网络开始，我们在不同的分布数据集上重新训练了许多DNN，并在完全d上评估时一致地观察到改进的OoD检测性能\n",
      "RGB-D显著性检测由于其有效性和现在可以方便地捕捉深度线索的事实而引起了越来越多的关注。现有的工作通常侧重于通过各种融合策略学习共享表示，很少有方法明确考虑如何保持模态特定的特征。在本文中，从一个新的角度，我们提出了一种用于RGB-D显著性检测的特异性保留网络，该网络通过探索共享信息和模态特定属性（例如，特异性）来提高显著性检测性能。具体而言，采用两个模态特定网络和一个共享学习网络来生成个体显著性图和共享显著性图。提出了一种交叉增强集成模块（CIM）来融合共享学习网络中的跨模态特征，然后将其传播到下一层以集成跨级别信息。此外，我们提出了一个多模态特征聚合（MFA）模块来实现\n",
      "三维点云视觉基础是一项新兴的视觉和语言任务，有利于理解三维视觉世界的各种应用。通过将这项任务定义为基于检测的问题，最近的许多工作都集中在如何利用更强大的检测器和全面的语言特征上，但（1）如何对复杂关系进行建模以生成上下文感知的对象建议，以及（2）如何利用建议关系将真实的目标对象与类似的建议区分开来，还没有得到充分的研究。受众所周知的transformer架构的启发，我们提出了一种基于3D点云的关系感知视觉基础方法，称为3DVG transformer，以充分利用上下文线索进行关系增强的提案生成和跨模态提案消歧，这是由在对象建议生成阶段新设计的坐标引导上下文聚合（CCA）模块和在跨模态featu中的多重注意力（MA）模块实现的\n",
      "自然图像的非局部自相似性已被广泛用于解决各种图像处理问题。当涉及到视频序列时，由于时间冗余，利用这种力量甚至更有益。在图像和视频去噪的背景下，许多面向经典的算法采用自相似性，将数据分割成重叠的补丁，收集相似的补丁组，并以某种方式将其处理在一起。随着卷积神经网络（CNN）的出现，基于补丁的框架已经被抛弃。大多数CNN去噪器对整个图像进行操作，仅通过使用大的接受场来隐含地利用非局部关系。这项工作提出了一种在视频去噪的背景下利用自相似性的新方法，同时仍然依赖于规则的卷积架构。我们引入了贴片工艺框架的概念——通过平铺匹配的贴片构建的与真实框架相似的人造框架。O\n",
      "近年来，基于文本的图像检索取得了长足的进步。然而，现有方法的性能在现实生活中受到影响，因为用户可能提供图像的不完整描述，这通常导致结果充满了符合不完整描述的误报。在这项工作中，我们介绍了部分查询问题，并广泛分析了它对基于文本的图像检索的影响。以前的交互式方法通过被动地接收用户的反馈来迭代地补充不完整的查询来解决这个问题，这是耗时的，并且需要大量的用户努力。相反，我们提出了一种新颖的检索框架，该框架以询问和确认的方式进行交互过程，人工智能主动搜索当前查询中缺失的歧视性细节，用户只需要确认人工智能的提议。具体来说，我们提出了一种基于对象的交互，使交互式检索更用户友好，并呈现出一种增强\n",
      "人工智能（AI）在从医疗保健到招聘的许多领域的成功部署需要负责任地使用，特别是在模型解释和隐私方面。可解释人工智能（XAI）提供了更多信息来帮助用户理解模型决策，但这些额外的知识暴露了隐私攻击的额外风险。因此，提供解释会损害隐私。我们研究了基于图像的模型反转攻击的这种风险，并确定了几种性能不断提高的攻击架构，以从模型解释中重建私人图像数据。我们已经开发了几种多模态转置CNN架构，它们实现了比仅使用目标模型预测高得多的反演性能。这些XAI感知反演模型旨在利用图像解释中的空间知识。为了了解哪些解释具有更高的隐私风险，我们分析了各种解释类型和因素如何影响inv\n",
      "训练用于识别与图像相关联的多个标签（包括识别看不见的标签）的神经网络模型是具有挑战性的，尤其是对于描绘许多语义不同标签的图像而言。尽管这项任务具有挑战性，但它是一项需要解决的重要任务，因为它代表了许多现实世界的情况，例如自然图像的图像检索。我们认为，像通常实践的那样，使用单个嵌入向量来表示图像不足以准确地对相关的可见和不可见标签进行排序。本研究介绍了一种用于多标签零样本学习的端到端模型训练，该训练支持图像和标签的语义多样性。我们建议使用具有使用定制损失函数训练的主嵌入向量的嵌入矩阵。此外，在训练过程中，我们建议在表现出更高语义多样性的损失函数图像样本中增加权重，以鼓励嵌入矩阵的多样性。广泛的实验sho\n",
      "现有的更改字幕研究主要集中在单个更改上。然而，检测和描述图像对中的多个变化部分对于增强对复杂场景的适应性至关重要。我们从三个方面解决了上述问题：（i）提出了一个基于模拟的多变化字幕数据集；（ii）我们将现有最先进的单次更改字幕方法与多次更改字幕进行比较；（iii）我们进一步提出了多变化字幕转换器（MCCFormer），其通过密集地关联图像对中的不同区域来识别变化区域，并动态地确定与句子中的单词相关的变化区域。所提出的方法在多变化字幕的四个传统变化字幕评估指标上获得了最高分数。此外，我们提出的方法可以为每个变化分离注意力图，并且在变化定位方面表现良好。此外，所提出的框架优于之前的统计数据\n",
      "从扫描设备获取的点云经常受到噪声的干扰，这会影响表面重建和分析等下游任务。噪声点云的分布可以看作是一组无噪声样本p（x）与某个噪声模型n卷积后的分布，从而得到（p*n）（x），其模式是下面的清洁表面。为了对噪声点云进行去噪，我们建议通过梯度上升来增加每个点的对数似然性，即迭代更新每个点的位置。由于p*n在测试时是未知的，并且我们只需要分数（即对数概率函数的梯度）来执行梯度上升，因此我们提出了一种神经网络架构来估计p*n的分数，仅给定噪声点云作为输入。我们推导出用于训练网络的目标函数，并利用估计的分数开发去噪算法。实验表明，在\n",
      "前所未有地获取多时相卫星图像为各种地球观测任务开辟了新的视角。其中，农业地块的像素精确全景分割具有重要的经济和环境影响。虽然研究人员已经在单个图像中探索了这个问题，但我们认为，通过图像的时间序列可以更好地解决作物表型的复杂时间模式。在本文中，我们提出了第一种端到端的单阶段卫星图像时间序列全景分割方法。该模块可以与我们新颖的图像序列编码网络相结合，该网络依赖于时间自注意来提取丰富且自适应的多尺度时空特征。我们还介绍了PASTIS，这是第一个具有全景注释的开放访问SITS数据集。我们展示了我们的编码器在语义分割方面相对于多种竞争网络架构的优势，并建立了第一个最先进的\n",
      "尽管卷积神经网络（CNNs）在计算机视觉领域取得了巨大成功，但这项工作研究了一种更简单、无卷积的主干网络，可用于许多密集的预测任务。与最近提出的专门为图像分类设计的视觉转换器（ViT）不同，我们引入了金字塔视觉转换器（PVT），它克服了将转换器移植到各种密集预测任务的困难。与现有技术相比，PVT有几个优点。（1） 与通常产生低分辨率输出并导致高计算和内存成本的ViT不同，PVT不仅可以在图像的密集分区上进行训练以实现高输出分辨率，这对密集预测很重要，而且还使用渐进收缩金字塔来减少大特征图的计算。（2） PVT继承了CNN和Transformer的优势，使其成为无卷积的各种视觉任务的统一骨干\n",
      "由于相机内部光线的意外反射和散射，因果拍摄的图像通常会出现光斑伪影。然而，由于光斑可能以各种形状、位置和颜色出现，因此从图像中完全检测和去除光斑是非常具有挑战性的。现有的方法依赖于光斑的预定义强度和几何先验，并且可能无法区分光源和光斑伪影之间的差异。我们观察到，图像中光源的条件在产生耀斑中起着重要作用。在本文中，我们提出了一个具有光源感知制导的深度框架，用于单图像光斑去除（SIFR）。特别地，我们首先分别检测光源区域和光斑区域，然后基于光源感知引导来去除光斑伪影。通过学习这两种区域之间的基本关系，我们的方法可以从图像中去除不同类型的光斑。此外，inste\n",
      "动态推理网络旨在提高计算效率，对给定的样本采用自适应执行路径。主流方法通常为每个卷积块分配一个路由器，并顺序地逐块执行决策，而不考虑动态推理过程中的关系。本文从路由器和样本两个方面对动态推理的关系进行了建模。我们设计了一种称为关系路由器的新型路由器，以对给定样本的路由器之间的关系进行建模。原则上，当前关系路由器通过图卷积聚合先前路由器的上下文特征，并将其路由器特征传播到后续路由器，从而以长距离的方式为当前块做出执行决策。此外，我们通过引入样本关系模块（SRM）来对样本之间的关系进行建模，鼓励相关样本沿着相关的执行路径前进。作为一个整体，我们称我们的方法\n",
      "本文介绍了一种用于训练参数变形形状生成器的无监督损失。关键思想是在生成的形状中强制保持局部刚性。我们的方法建立在尽可能刚性（或ARAP）变形能量的局部近似上。我们展示了如何通过ARAP损失的Hessian谱分解来发展无监督损失。我们的损失通过一个稳健的范数很好地解耦了姿势和形状的变化。损失承认了简单的封闭形式表达。它易于训练，并且可以插入任何标准的一代模型，例如VAE和GAN。实验结果表明，在DFAUST、Animal和Bone等各种数据集中，我们的方法显著优于现有的形状生成方法。\n",
      "我们提出了一个用于弱监督对象定位（WSOL）的两阶段学习框架。虽然以前的大多数工作都依赖于基于高级特征的CAM（类激活映射），但本文提出使用基于低级特征的激活映射来定位对象。在第一阶段中，激活图生成器基于分类器中的低级特征图来生成激活图，使得以在线方式包括丰富的上下文对象信息。在第二阶段，我们使用评估器来评估由激活图生成器预测的激活图。在此基础上，我们进一步提出了加权熵损失、注意擦除和面积损失，以驱动激活图生成器大幅降低对象和背景之间激活的不确定性，并探索歧视性较小的区域。第二阶段模型在第一阶段保存的低层次对象信息的基础上，逐渐生成一个分离良好的、复杂的、可扩展的对象\n"
     ]
    }
   ],
   "source": [
    "for ab in abstracts:\n",
    "    trans = translate_api(ab)\n",
    "    ch_abstracts.append(trans['trans_result'][0]['dst'])\n",
    "    time.sleep(2)\n",
    "write2txt(\"./my_abstract.txt\",name,ch_abstracts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
